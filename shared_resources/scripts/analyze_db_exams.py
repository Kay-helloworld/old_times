import os
import re
from collections import defaultdict
import pypdf

DB_EXAMS_DIR = "exams/db"
PROCESSED_DB_DIR = "processed/db"
OUTPUT_FILE = "db_knowledge_analysis.md"

# Keywords Configuration
CATEGORIES = {
    "1. SQL & æŸ¥è©¢ (SQL & Queries)": [
        "SQL", "SELECT", "UPDATE", "DELETE", "INSERT", "View", "Trigger", "Stored Procedure", "Cursor", "Join"
    ],
    "2. æ­£è¦åŒ– (Normalization)": [
        "Normalization", "æ­£è¦åŒ–", "1NF", "2NF", "3NF", "BCNF", "Functional Dependency", "åŠŸèƒ½ç›¸ä¾", "Lossless", "ç„¡å¤±çœŸ"
    ],
    "3. äº¤æ˜“ç®¡ç† (Transaction Management)": [
        "Transaction", "äº¤æ˜“", "ACID", "Concurrency", "ä¸¦è¡Œ", "Lock", "é–å®š", "Deadlock", "æ­»çµ", "Isolation", "éš”é›¢", "Recovery", "å¾©åŸ", "Log", "æ—¥èªŒ", "Checkpoint", "æª¢æŸ¥é»"
    ],
    "4. è³‡æ–™åº«è¨­è¨ˆ (DB Design)": [
        "ER Model", "ERD", "Entity", "å¯¦é«”", "Relationship", "é—œè¯", "Schema", "ç¶±è¦", "Constraint", "é™åˆ¶"
    ],
    "5. ç´¢å¼•èˆ‡å„²å­˜ (Indexing & Storage)": [
        "Index", "ç´¢å¼•", "B-Tree", "B+ Tree", "Hashing", "Hash", "é›œæ¹Š", "RAID"
    ],
    "6. é€²éšä¸»é¡Œ (Advanced Topics)": [
        "Distributed", "åˆ†æ•£å¼", "NoSQL", "Big Data", "å¤§æ•¸æ“š", "Data Warehouse", "è³‡æ–™å€‰å„²", "Data Mining", "è³‡æ–™æ¢å‹˜", "OLAP"
    ],
    "7. è³‡è¨Šå®‰å…¨ (Security)": [
        "Security", "è³‡å®‰", "Encryption", "åŠ å¯†", "Decryption", "è§£å¯†", "Authentication", "èªè­‰", "Authorization", "æˆæ¬Š", "Injection", "éš±ç¢¼"
    ]
}

SPECIAL_TOPICS = {
    "è³‡è¨Šå®‰å…¨ (Security)": ["Security", "è³‡å®‰", "Encryption", "åŠ å¯†", "Hacking", "é§­å®¢", "Injection", "éš±ç¢¼"],
    "äººå·¥æ™ºæ…§ (AI)": ["Artificial Intelligence", "äººå·¥æ™ºæ…§", "Machine Learning", "æ©Ÿå™¨å­¸ç¿’", "Deep Learning", "æ·±åº¦å­¸ç¿’"],
    "å¤§æ•¸æ“š (Big Data)": ["Big Data", "å¤§æ•¸æ“š", "Hadoop", "Spark", "MapReduce"]
}

def extract_text(filepath):
    try:
        reader = pypdf.PdfReader(filepath)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        print(f"Error reading {filepath}: {e}")
        return ""

def get_year_from_filename(filename):
    # Extract year (e.g., "114å¹´..." -> 114, "1 1 3å¹´..." -> 113)
    clean_name = filename.replace(" ", "")
    match = re.search(r'(\d{3})å¹´', clean_name)
    if match:
        return int(match.group(1))
    return 0

def analyze_subset(files, subset_name):
    category_counts = defaultdict(int)
    special_counts = defaultdict(int)
    special_locations = defaultdict(list)
    
    print(f"Analyzing subset: {subset_name} ({len(files)} files)...")

    for filename in files:
        filepath = os.path.join(PROCESSED_DB_DIR, filename)
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read().lower()
            
            # Categories
            for category, terms in CATEGORIES.items():
                count = 0
                for term in terms:
                    count += content.count(term.lower())
                if count > 0:
                    category_counts[category] += count
            
            # Special Topics
            for topic, terms in SPECIAL_TOPICS.items():
                count = 0
                for term in terms:
                    count += content.count(term.lower())
                if count > 0:
                    special_counts[topic] += count
                    special_locations[topic].append(filename.replace(".txt", ""))
                    
        except Exception as e:
            print(f"Error analyzing {filename}: {e}")
            
    return category_counts, special_counts, special_locations

def main():
    if not os.path.exists(PROCESSED_DB_DIR):
        os.makedirs(PROCESSED_DB_DIR)

    # 1. Extract Text
    pdf_files = [f for f in os.listdir(DB_EXAMS_DIR) if f.lower().endswith(".pdf")]
    print(f"Found {len(pdf_files)} PDFs. Checking for extracted text...")
    
    for pdf_file in pdf_files:
        txt_filename = f"{os.path.splitext(pdf_file)[0]}.txt"
        txt_path = os.path.join(PROCESSED_DB_DIR, txt_filename)
        
        if not os.path.exists(txt_path):
            print(f"Extracting {pdf_file}...")
            text = extract_text(os.path.join(DB_EXAMS_DIR, pdf_file))
            with open(txt_path, "w", encoding="utf-8") as f:
                f.write(text)

    # 2. Analyze
    txt_files = [f for f in os.listdir(PROCESSED_DB_DIR) if f.endswith(".txt")]
    
    # Filter Subsets
    local_gov_files = [f for f in txt_files if "åœ°æ–¹æ”¿åºœ" in f]
    higher_exam_files = [f for f in txt_files if "é«˜ç­‰è€ƒè©¦ä¸‰ç´š" in f]
    recent_files = [f for f in txt_files if get_year_from_filename(f) in [112, 113, 114]]
    files_114 = [f for f in txt_files if get_year_from_filename(f) == 114]
    
    results = {}
    results['all'] = analyze_subset(txt_files, "All Exams")
    results['local'] = analyze_subset(local_gov_files, "Local Gov Exams")
    results['high'] = analyze_subset(higher_exam_files, "Higher Exams L3")
    results['recent'] = analyze_subset(recent_files, "Recent 3 Years (112-114)")
    results['114'] = analyze_subset(files_114, "114 Only")
    
    # 3. Generate Report
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("# è³‡æ–™åº«æ‡‰ç”¨ - æ­·å±†è€ƒé¡ŒçŸ¥è­˜é»åˆ†æå ±å‘Š\n\n")
        f.write(f"**åˆ†ææª”æ¡ˆæ•¸é‡**: {len(txt_files)} ä»½\n\n")
        
        # Part 1: Special Topics
        f.write("## ğŸš€ ç‰¹æ®Šä¸»é¡Œåˆ†æ (è³‡å®‰ã€AIã€å¤§æ•¸æ“š)\n")
        f.write("åµæ¸¬é€™äº›æ–°èˆˆæˆ–è·¨é ˜åŸŸä¸»é¡Œçš„å‡ºç¾é »ç‡èˆ‡ä½ç½®ã€‚\n\n")
        
        all_special = results['all'][1]
        all_locs = results['all'][2]
        
        for topic, count in all_special.items():
            f.write(f"### {topic}: {count} æ¬¡\n")
            if count > 0:
                f.write("**å‡ºç¾è€ƒå·**:\n")
                for loc in all_locs[topic]:
                    f.write(f"- {loc}\n")
            f.write("\n")
            
        f.write("---\n\n")
        
        # Part 2: Trend Comparison (New!)
        f.write("## ğŸ“ˆ è¿‘ä¸‰å¹´è¶¨å‹¢åˆ†æ (112-114 vs å…¨éƒ¨)\n")
        f.write("æ¯”è¼ƒè¿‘ä¸‰å¹´çš„è€ƒé»åˆ†å¸ƒèˆ‡æ­·å¹´æ•´é«”è¶¨å‹¢ã€‚\n\n")
        
        f.write("| çŸ¥è­˜é»é¡åˆ¥ | æ­·å¹´å…¨éƒ¨ (All) | è¿‘ä¸‰å¹´ (112-114) | 114å¹´ |\n")
        f.write("| :--- | :---: | :---: | :---: |\n")
        
        sorted_cats = sorted(CATEGORIES.keys())
        for cat in sorted_cats:
            c_all = results['all'][0].get(cat, 0)
            c_recent = results['recent'][0].get(cat, 0)
            c_114 = results['114'][0].get(cat, 0)
            f.write(f"| {cat} | {c_all} | {c_recent} | {c_114} |\n")
            
        f.write("\n---\n\n")
        
        # Part 3: Comparative Analysis (Exam Types)
        f.write("## ğŸ“Š è€ƒé»é »ç‡æ¯”è¼ƒ (å…¨éƒ¨ vs åœ°ç‰¹ vs é«˜è€ƒ)\n")
        f.write("æ¯”è¼ƒä¸åŒè€ƒè©¦é¡å‹çš„å‡ºé¡Œé‡å¿ƒã€‚\n\n")
        
        f.write("| çŸ¥è­˜é»é¡åˆ¥ | å…¨éƒ¨ (All) | åœ°æ–¹æ”¿åºœ (Local) | é«˜è€ƒä¸‰ç´š (High) |\n")
        f.write("| :--- | :---: | :---: | :---: |\n")
        
        for cat in sorted_cats:
            c_all = results['all'][0].get(cat, 0)
            c_local = results['local'][0].get(cat, 0)
            c_high = results['high'][0].get(cat, 0)
            f.write(f"| {cat} | {c_all} | {c_local} | {c_high} |\n")
            
        f.write("\n---\n\n")
        
        # Part 4: Detailed Breakdown
        f.write("## ğŸ“ è©³ç´°è€ƒé»é—œéµå­—\n")
        for cat in sorted_cats:
            f.write(f"- **{cat}**: {', '.join(CATEGORIES[cat])}\n")

    print(f"Analysis complete. Report generated at {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
