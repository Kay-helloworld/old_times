import os
import re
from collections import defaultdict
import pypdf

PROCESSED_DB_DIR = "database_application/exam_resources/processed_text/db/db"
OUTPUT_FILE = "database_application/exam_resources/analysis_reports/db_knowledge_analysis_v2.md"

# æ”¹é€²çš„é—œéµå­—é…ç½® - æŒ‰ç·´ç¿’æ–¹å¼åˆ†é¡
CATEGORIES = {
    "1. SQLå¯¦ä½œ (SQL Practice)": [
        # DDL
        "CREATE TABLE", "ALTER TABLE", "DROP TABLE",
        "PRIMARY KEY", "FOREIGN KEY", "REFERENCES",
        # DML
        "SELECT", "INSERT", "UPDATE", "DELETE",
        "JOIN", "INNER JOIN", "LEFT JOIN", "RIGHT JOIN",
        "GROUP BY", "HAVING", "ORDER BY",
        "WHERE", "DISTINCT", "UNION",
        # èšåˆå‡½æ•¸
        "COUNT", "SUM", "AVG", "MAX", "MIN",
        # é€²éšSQL
        "VIEW", "CREATE VIEW", "è¦–åœ–",
        "TRIGGER", "è§¸ç™¼å™¨",
        "STORED PROCEDURE", "é å­˜ç¨‹åº",
        "CURSOR", "æ¸¸æ¨™",
        "SUBQUERY", "å­æŸ¥è©¢", "IN (SELECT", "EXISTS"
    ],
    
    "2. è³‡æ–™åº«è¨­è¨ˆ (DB Design)": [
        # ER Modelæ ¸å¿ƒ
        "ER Model", "ERD", "ER Diagram", "å¯¦é«”é—œä¿‚åœ–",
        "Entity-Relationship", "E-R Model",
        # EER
        "EER", "Enhanced ER", "Enhanced Entity", "æ“´å……å¯¦é«”é—œä¿‚",
        # ERå…ƒç´ 
        "Entity Type", "å¯¦é«”å‹æ…‹",
        "Relationship Type", "é—œè¯å‹æ…‹",
        "Weak Entity", "å¼±å¯¦é«”",
        "Identifying Relationship",
        # åŸºæ•¸èˆ‡åƒèˆ‡
        "Cardinality", "åŸºæ•¸", "Multiplicity",
        "Participation", "åƒèˆ‡",
        "One-to-One", "1:1", "ä¸€å°ä¸€",
        "One-to-Many", "1:N", "ä¸€å°å¤š",
        "Many-to-Many", "M:N", "å¤šå°å¤š",
        # ç¹¼æ‰¿
        "Supertype", "Subtype", "è¶…é¡åˆ¥", "å­é¡åˆ¥",
        "Specialization", "ç‰¹æ®ŠåŒ–",
        "Generalization", "ä¸€èˆ¬åŒ–",
        "ISA", "is-a",
        # èšåˆ
        "Aggregation", "èšåˆ",
        # å±¬æ€§
        "Composite Attribute", "è¤‡åˆå±¬æ€§",
        "Multivalued Attribute", "å¤šå€¼å±¬æ€§",
        "Derived Attribute", "è¡ç”Ÿå±¬æ€§",
        # è½‰æ›
        "Mapping", "å°æ˜ ",
        "Relational Schema", "é—œè¯ç¶±è¦"
    ],
    
    "3. æ­£è¦åŒ– (Normalization)": [
        # æ­£è¦åŒ–
        "Normalization", "æ­£è¦åŒ–", "Normal Form",
        "1NF", "First Normal Form", "ç¬¬ä¸€æ­£è¦",
        "2NF", "Second Normal Form", "ç¬¬äºŒæ­£è¦",
        "3NF", "Third Normal Form", "ç¬¬ä¸‰æ­£è¦",
        "BCNF", "Boyce-Codd",
        "4NF", "Fourth Normal Form",
        # åŠŸèƒ½ç›¸ä¾
        "Functional Dependency", "åŠŸèƒ½ç›¸ä¾", "FD",
        "Partial Dependency", "éƒ¨åˆ†ç›¸ä¾",
        "Transitive Dependency", "éç§»ç›¸ä¾",
        "Multivalued Dependency", "å¤šå€¼ç›¸ä¾", "MVD",
        # æ¨å°
        "Closure", "å°é–‰",
        "Armstrong", "Armstrong's Axioms",
        # éµå€¼
        "Candidate Key", "å€™é¸éµ",
        "Prime Attribute",
        "Non-Prime Attribute",
        "Superkey", "è¶…éµ",
        # åˆ†è§£
        "Decomposition", "åˆ†è§£",
        "Lossless Join", "ç„¡å¤±çœŸ", "ç„¡æé€£æ¥",
        "Dependency Preserving"
    ],
    
    "4. äº¤æ˜“ç®¡ç† (Transaction Management)": [
        # äº¤æ˜“æ ¸å¿ƒ
        "Transaction", "äº¤æ˜“", "äº‹å‹™",
        "ACID",
        "Atomicity", "Consistency", "Isolation", "Durability",
        "åŸå­æ€§", "ä¸€è‡´æ€§", "éš”é›¢æ€§", "æŒä¹…æ€§",
        # ä¸¦è¡Œæ§åˆ¶
        "Concurrency Control", "ä¸¦è¡Œ", "ä¸¦ç™¼",
        "Schedule", "æ’ç¨‹",
        "Serial", "Serializable", "å¯åºåˆ—",
        "Conflict Serializable",
        "View Serializable",
        # é–å®š
        "Lock", "é–å®š", "Locking",
        "Shared Lock", "S-Lock",
        "Exclusive Lock", "X-Lock",
        "Two-Phase Locking", "2PL", "å…©éšæ®µé–",
        "Deadlock", "æ­»çµ", "æ­»é–",
        "Wait-for Graph",
        "Timestamp", "æ™‚é–“æˆ³",
        # éš”é›¢ç´šåˆ¥
        "Isolation Level",
        "Read Uncommitted", "Read Committed",
        "Repeatable Read",
        # å•é¡Œ
        "Dirty Read", "é«’è®€",
        "Non-Repeatable Read",
        "Phantom Read", "å¹»è®€",
        "Lost Update",
        # å¾©åŸ
        "Recovery", "å¾©åŸ", "Recover",
        "Log", "æ—¥èªŒ", "Logging",
        "Checkpoint", "æª¢æŸ¥é»",
        "Undo", "Redo",
        "Write-Ahead Log", "WAL",
        "Commit", "Rollback"
    ],
    
    "5. ç´¢å¼•èˆ‡å„²å­˜ (Indexing & Storage)": [
        # ç´¢å¼•
        "Index", "ç´¢å¼•", "Indexing",
        "Clustered Index", "å¢é›†ç´¢å¼•",
        "Non-Clustered Index",
        "Secondary Index",
        # Bæ¨¹ï¼ˆåªåœ¨è³‡æ–™åº«contextï¼‰
        "B-Tree", "B Tree", "Bæ¨¹",
        "B+Tree", "B+ Tree", "B+æ¨¹",
        "B*Tree",
        # Hash
        "Hash Index", "é›œæ¹Šç´¢å¼•",
        "Hash Function", "é›œæ¹Šå‡½æ•¸",
        "Bucket", "æ¡¶",
        "Linear Hashing",
        "Extendible Hashing",
        # å„²å­˜
        "Storage", "å„²å­˜",
        "File Organization",
        "Heap File", "å †ç©æª”",
        "Sequential File",
        "Buffer", "ç·©è¡å€",
        # RAID
        "RAID",
        "Striping", "Mirroring", "Parity"
    ],
    
    "6. é€²éšä¸»é¡Œ (Advanced Topics)": [
        # åˆ†æ•£å¼
        "Distributed Database", "åˆ†æ•£å¼è³‡æ–™åº«",
        "Fragmentation", "ç‰‡æ®µåŒ–",
        "Replication", "è¤‡è£½",
        "Two-Phase Commit", "2PC",
        "CAP Theorem",
        # NoSQL
        "NoSQL",
        "Key-Value Store", "Document Store",
        "Column-Family", "Graph Database",
        "MongoDB", "Redis", "Cassandra",
        "BASE", "Eventually Consistent",
        # Big Data
        "Big Data", "å¤§æ•¸æ“š",
        "Hadoop", "MapReduce", "Spark", "HDFS",
        "Data Lake",
        # Data Warehouse
        "Data Warehouse", "è³‡æ–™å€‰å„²",
        "OLAP", "OLTP",
        "Data Mart",
        "Star Schema", "Snowflake Schema",
        "Fact Table", "Dimension Table",
        "ETL",
        # Data Mining
        "Data Mining", "è³‡æ–™æ¢å‹˜",
        "Association Rule", "é—œè¯è¦å‰‡",
        "Classification", "åˆ†é¡",
        "Clustering", "åˆ†ç¾¤"
    ],
    
    "7. è³‡è¨Šå®‰å…¨ (Security)": [
        # å®‰å…¨æ ¸å¿ƒ
        "Security", "è³‡å®‰", "å®‰å…¨æ€§",
        "Information Security",
        # åŠ å¯†
        "Encryption", "åŠ å¯†",
        "Decryption", "è§£å¯†",
        "Cryptography", "å¯†ç¢¼å­¸",
        "Symmetric", "Asymmetric",
        "Public Key", "Private Key",
        # èªè­‰æˆæ¬Š
        "Authentication", "èªè­‰",
        "Authorization", "æˆæ¬Š",
        "Access Control", "å­˜å–æ§åˆ¶",
        "RBAC", "DAC", "MAC",
        "Grant", "Revoke",
        # æ”»æ“Šé˜²è­·
        "SQL Injection", "SQLéš±ç¢¼",
        "Injection Attack",
        "Prepared Statement",
        "Parameterized Query",
        # ç¨½æ ¸
        "Audit", "ç¨½æ ¸", "Auditing"
    ]
}

def get_year_from_filename(filename):
    clean_name = filename.replace(" ", "")
    match = re.search(r'(\d{3})å¹´', clean_name)
    if match:
        return int(match.group(1))
    return 0

def analyze_subset(files, subset_name):
    category_counts = defaultdict(int)
    
    print(f"åˆ†æ {subset_name}: {len(files)} ä»½è€ƒå·...")

    for filename in files:
        filepath = os.path.join(PROCESSED_DB_DIR, filename)
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read().lower()
            
            for category, terms in CATEGORIES.items():
                count = 0
                for term in terms:
                    count += content.count(term.lower())
                if count > 0:
                    category_counts[category] += count
                    
        except Exception as e:
            print(f"âœ— Error: {filename}: {e}")
            
    return category_counts

def main():
    # æª¢æŸ¥ç›®éŒ„
    if not os.path.exists(PROCESSED_DB_DIR):
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ç›®éŒ„ {PROCESSED_DB_DIR}")
        return
    
    txt_files = [f for f in os.listdir(PROCESSED_DB_DIR) if f.endswith(".txt")]
    print(f"\næ‰¾åˆ° {len(txt_files)} ä»½è³‡æ–™åº«æ‡‰ç”¨è€ƒé¡Œ\n")
    
    # åˆ†é¡åˆ†æ
    local_gov_files = [f for f in txt_files if "åœ°æ–¹æ”¿åºœ" in f]
    higher_exam_files = [f for f in txt_files if "é«˜ç­‰è€ƒè©¦ä¸‰ç´š" in f]
    recent_files = [f for f in txt_files if get_year_from_filename(f) in [112, 113, 114]]
    files_114 = [f for f in txt_files if get_year_from_filename(f) == 114]
    
    results = {}
    results['all'] = analyze_subset(txt_files, "å…¨éƒ¨è€ƒå·")
    results['local'] = analyze_subset(local_gov_files, "åœ°æ–¹æ”¿åºœ")
    results['high'] = analyze_subset(higher_exam_files, "é«˜è€ƒä¸‰ç´š")
    results['recent'] = analyze_subset(recent_files, "è¿‘ä¸‰å¹´(112-114)")
    results['114'] = analyze_subset(files_114, "114å¹´")
    
    # ç”Ÿæˆå ±å‘Š
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("# è³‡æ–™åº«æ‡‰ç”¨ - çŸ¥è­˜é»åˆ†æå ±å‘Š (æ”¹é€²ç‰ˆ v2)\n\n")
        f.write(f"**åˆ†ææª”æ¡ˆæ•¸é‡**: {len(txt_files)} ä»½\n\n")
        f.write("**åˆ†ææ–¹æ³•**: æŒ‰ç·´ç¿’æ–¹å¼åˆ†é¡ï¼ˆä¸Šæ©Ÿå¯¦ä½œ vs ç´™ç­†æ¨å° vs ç†è«–æ¦‚å¿µï¼‰\n\n")
        f.write("**å…è¨±é‡è¤‡è¨ˆç®—**: è¤‡åˆé¡Œç›®æœƒåŒæ™‚å‡ºç¾åœ¨å¤šå€‹åˆ†é¡ï¼ˆé€™æ˜¯åˆç†çš„ï¼‰\n\n")
        
        f.write("---\n\n")
        
        # è¶¨å‹¢åˆ†æ
        f.write("## ğŸ“ˆ è¿‘ä¸‰å¹´è¶¨å‹¢åˆ†æ (112-114 vs å…¨éƒ¨)\n\n")
        f.write("| çŸ¥è­˜é»é¡åˆ¥ | æ­·å¹´å…¨éƒ¨ | è¿‘ä¸‰å¹´ | 114å¹´ | è¿‘ä¸‰å¹´ä½”æ¯” |\n")
        f.write("| :--- | :---: | :---: | :---: | :---: |\n")
        
        sorted_cats = sorted(CATEGORIES.keys())
        for cat in sorted_cats:
            c_all = results['all'].get(cat, 0)
            c_recent = results['recent'].get(cat, 0)
            c_114 = results['114'].get(cat, 0)
            ratio = f"{c_recent/c_all*100:.1f}%" if c_all > 0 else "0%"
            f.write(f"| {cat} | {c_all} | {c_recent} | {c_114} | {ratio} |\n")
            
        f.write("\n---\n\n")
        
        # è€ƒè©¦é¡å‹æ¯”è¼ƒ
        f.write("## ğŸ“Š è€ƒè©¦é¡å‹æ¯”è¼ƒ (å…¨éƒ¨ vs åœ°ç‰¹ vs é«˜è€ƒ)\n\n")
        f.write("| çŸ¥è­˜é»é¡åˆ¥ | å…¨éƒ¨ | åœ°æ–¹æ”¿åºœ | é«˜è€ƒä¸‰ç´š |\n")
        f.write("| :--- | :---: | :---: | :---: |\n")
        
        for cat in sorted_cats:
            c_all = results['all'].get(cat, 0)
            c_local = results['local'].get(cat, 0)
            c_high = results['high'].get(cat, 0)
            f.write(f"| {cat} | {c_all} | {c_local} | {c_high} |\n")
            
        f.write("\n---\n\n")
        
        # è©³ç´°é—œéµå­—
        f.write("## ğŸ“ è©³ç´°è€ƒé»é—œéµå­—\n\n")
        for cat in sorted_cats:
            f.write(f"### {cat}\n\n")
            f.write(f"```\n{', '.join(CATEGORIES[cat])}\n```\n\n")
            
        f.write("---\n\n")
        f.write("## ğŸ’¡ èªªæ˜\n\n")
        f.write("- **åˆ†é¡åŸå‰‡**: æŒ‰ç·´ç¿’æ–¹å¼åˆ†é¡ï¼Œè€Œéå‚³çµ±å­¸è¡“åˆ†é¡\n")
        f.write("- **é‡è¤‡è¨ˆç®—**: ä¸€å€‹è¤‡åˆé¡Œå¯èƒ½åŒæ™‚åŒ…å«SQLå’Œè¨­è¨ˆï¼Œæœƒè¢«è¨ˆç®—å…©æ¬¡\n")
        f.write("- **é—œéµå­—é¸æ“‡**: æ›´æ˜ç¢ºçš„è©å½™ï¼Œæ¸›å°‘èª¤åˆ¤\n")
        f.write("- **è³‡æ–™ä¾†æº**: åƒ…é™è³‡æ–™åº«æ‡‰ç”¨ç§‘ç›®è€ƒå·ï¼Œä¸å«è³‡æ–™çµæ§‹è€ƒå·\n")

    print(f"\nâœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ“„ å ±å‘Šå·²ç”Ÿæˆ: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
