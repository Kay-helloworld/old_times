import os
import re
import glob
import json
from collections import Counter
import jieba
import jieba.analyse

# Configuration
SOURCE_DIR = "/Users/kaylo/Documents/ç¨‹å¼ç›¸é—œ/antigravity/information_management/exam_resources/processed_text"
OUTPUT_FILE = "/Users/kaylo/Documents/ç¨‹å¼ç›¸é—œ/antigravity/information_management/essay_guides/local_gov_exam_questions.md"

# Custom dictionary for information management terms
CUSTOM_DICT = [
    # è³‡è¨Šå®‰å…¨
    "è³‡è¨Šå®‰å…¨", "è³‡å®‰", "ISMS", "åŠ å¯†", "Encryption", "å¯†ç¢¼å­¸", "Cryptography",
    "é˜²ç«ç‰†", "Firewall", "IDS", "IPS", "VPN", "DDoS", "SQL Injection",
    "XSS", "CSRF", "é›¶ä¿¡ä»»", "Zero Trust", "PKI", "SSL", "TLS",
    # AI & ML
    "äººå·¥æ™ºæ…§", "AI", "æ©Ÿå™¨å­¸ç¿’", "Machine Learning", "æ·±åº¦å­¸ç¿’", "Deep Learning",
    "ç¥ç¶“ç¶²è·¯", "Neural Network", "ChatGPT", "GPT", "LLM", "NLP",
    # é›²ç«¯é‹ç®—
    "é›²ç«¯é‹ç®—", "Cloud Computing", "IaaS", "PaaS", "SaaS",
    "AWS", "Azure", "GCP", "å¾®æœå‹™", "Microservices",
    # è³‡æ–™åº«
    "è³‡æ–™åº«", "Database", "SQL", "NoSQL", "RDBMS",
    "æ­£è¦åŒ–", "Normalization", "ER Model", "Transaction", "ACID",
    # ERP & ç®¡ç†
    "ERP", "CRM", "SCM", "Supply Chain", "Enterprise Resource Planning",
    "BPR", "Business Process", "KM", "Knowledge Management",
    # IoT & 5G
    "ç‰©è¯ç¶²", "IoT", "5G", "æ„Ÿæ¸¬å™¨", "Sensor", "RFID", "Edge Computing",
    # ç³»çµ±é–‹ç™¼
    "SDLC", "Agile", "Scrum", "DevOps", "UML", "Waterfall",
    "Software Testing", "CI/CD", "Version Control",
    # è³‡æ–™åˆ†æ
    "Big Data", "å¤§æ•¸æ“š", "Data Mining", "è³‡æ–™æ¢å‹˜", "ETL",
    "Business Intelligence", "BI", "OLAP", "Data Warehouse",
    # å°ˆæ¡ˆç®¡ç†
    "å°ˆæ¡ˆç®¡ç†", "Project Management", "PMBOK", "Gantt Chart",
    "PERT", "CPM", "WBS", "ROI", "NPV",
    # ITæ²»ç†
    "ITæ²»ç†", "COBIT", "ITIL", "ISO 20000", "SLA",
    "ç¨½æ ¸", "Audit", "Compliance", "BSC", "KPI",
    # å€å¡Šéˆ
    "å€å¡Šéˆ", "Blockchain", "Bitcoin", "Cryptocurrency", "Smart Contract",
    "PoW", "PoS", "NFT", "Metaverse",
    # é›»å­å•†å‹™
    "é›»å­å•†å‹™", "E-Commerce", "Digital Marketing", "SEO", "SEM",
    "ç¤¾ç¾¤åª’é«”", "Social Media", "O2O", "UX", "UI",
    # ç­–ç•¥ç®¡ç†
    "ç«¶çˆ­ç­–ç•¥", "Porter", "SWOT", "åƒ¹å€¼éˆ", "Value Chain",
    "è—æµ·ç­–ç•¥", "æ•¸ä½è½‰å‹", "Digital Transformation",
]

def setup_jieba():
    for word in CUSTOM_DICT:
        jieba.add_word(word)

def extract_year(filename):
    # Handle "104050_1301" format
    match = re.match(r'(\d{3})\d{3}_', filename)
    if match:
        return int(match.group(1))
    return 0

def is_target_file(filename, content):
    # Check for "åœ°æ–¹æ”¿åºœ" or "åœ°æ–¹ç‰¹è€ƒ" and "ä¸‰ç­‰"
    if "åœ°æ–¹" in content and "ä¸‰ç­‰" in content:
        return True
    return False

def extract_questions_from_file(filepath):
    filename = os.path.basename(filepath)
    year = extract_year(filename)
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Check if this is a target file
        if not is_target_file(filename, content):
            return []
        
        # Extract metadata
        metadata = {'year': year, 'filename': filename}
        
        # Extract exam type
        if 'é«˜ç­‰è€ƒè©¦' in content:
            metadata['exam_type'] = 'é«˜ç­‰è€ƒè©¦'
        elif 'åœ°æ–¹æ”¿åºœ' in content or 'åœ°æ–¹ç‰¹è€ƒ' in content:
            metadata['exam_type'] = 'åœ°æ–¹ç‰¹è€ƒ'
        
        # Extract level
        level_match = re.search(r'(ä¸‰ç­‰|äºŒç­‰|å››ç­‰|äº”ç­‰)', content[:500])
        if level_match:
            metadata['level'] = level_match.group(1)
        
        # Extract subject
        subject_match = re.search(r'ç§‘ ç›®[ï¼š:]\s*(.+)', content[:500])
        if subject_match:
            metadata['subject'] = subject_match.group(1).strip()
        
        # Parse questions using Chinese numerals
        question_pattern = r'^(ä¸€|äºŒ|ä¸‰|å››|äº”)ã€'
        lines = content.split('\n')
        
        questions = []
        current_question = None
        current_number = None
        
        for line in lines:
            line = line.strip()
            # Skip metadata lines
            if any(skip in line for skip in ['ä»£è™Ÿï¼š', 'é æ¬¡ï¼š', 'â€»æ³¨æ„', 'åº§è™Ÿï¼š', 'è€ƒ è©¦ åˆ¥', 'ç­‰ åˆ¥', 'é¡ ç§‘', 'ç§‘ ç›®', 'è€ƒè©¦æ™‚é–“']):
                continue
            
            match = re.match(question_pattern, line)
            if match:
                # Save previous question
                if current_question:
                    questions.append({
                        'year': year,
                        'number': current_number,
                        'content': '\n'.join(current_question),
                        'metadata': metadata
                    })
                
                # Start new question
                current_number = match.group(1)
                current_question = [line]
            elif current_question is not None and line:
                current_question.append(line)
        
        # Add last question
        if current_question:
            questions.append({
                'year': year,
                'number': current_number,
                'content': '\n'.join(current_question),
                'metadata': metadata
            })
            
    except Exception as e:
        print(f"Error reading {filename}: {e}")
        return []
        
    return questions

def extract_keywords(text, top_k=10):
    keywords = jieba.analyse.extract_tags(text, topK=top_k, allowPOS=('n', 'eng', 'v', 'vn'))
    filtered = [k for k in keywords if len(k) > 1 or k.upper() in CUSTOM_DICT]
    return filtered

def main():
    setup_jieba()
    
    all_questions = []
    files = glob.glob(os.path.join(SOURCE_DIR, "*.txt"))
    
    print(f"æƒæ {len(files)} å€‹æª”æ¡ˆ...")
    
    for filepath in files:
        qs = extract_questions_from_file(filepath)
        all_questions.extend(qs)
        if qs:
            print(f"  âœ“ {os.path.basename(filepath)}: {len(qs)} é¡Œ")
    
    # Sort questions by year (descending)
    all_questions.sort(key=lambda x: x['year'], reverse=True)
    
    print(f"\nç¸½å…±æ‰¾åˆ° {len(all_questions)} é¡Œåœ°æ–¹ç‰¹è€ƒä¸‰ç­‰è³‡è¨Šç®¡ç†è©¦é¡Œ")
    
    # Analyze keywords
    all_keywords = []
    
    for q in all_questions:
        text = q['content']
        keywords = extract_keywords(text)
        
        # Manual check for custom terms
        for term in CUSTOM_DICT:
            if term.lower() in text.lower() and term not in keywords:
                keywords.append(term)
        
        q['keywords'] = keywords
        all_keywords.extend(keywords)
        
    keyword_counts = Counter(all_keywords)
    
    # Generate Markdown
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write("# åœ°æ–¹ç‰¹è€ƒä¸‰ç´š è³‡è¨Šç®¡ç† æ­·å±†è©¦é¡Œå½™æ•´èˆ‡åˆ†æ\n\n")
        f.write("> æœ¬æ–‡ä»¶å½™æ•´ **åœ°æ–¹ç‰¹è€ƒä¸‰ç´š** è³‡è¨Šç®¡ç†ç§‘ç›®æ­·å±†è€ƒé¡Œï¼Œæä¾›å®Œæ•´åŸé¡Œèˆ‡é—œéµå­—åˆ†æï¼Œæ˜¯å‚™è€ƒè³‡è¨Šç®¡ç†è€ƒè©¦çš„é‡è¦åƒè€ƒè³‡æºã€‚\n\n")
        f.write("---\n\n")
        
        # Statistics section will be added separately
        f.write("## 1. ğŸ“Š é¡Œç›®ç¸½è¦½\n\n")
        f.write("*çµ±è¨ˆæ•¸æ“šæº–å‚™ä¸­...*\n\n")
        f.write("---\n\n")
        
        f.write("## 2. é—œéµå­—åˆ†æ (Keyword Analysis)\n\n")
        f.write("ä»¥ä¸‹ç‚ºæ­·å±†è©¦é¡Œä¸­å‡ºç¾é »ç‡æœ€é«˜çš„é—œéµå­—ï¼Œå¯ä½œç‚ºé‡é»è¤‡ç¿’æ–¹å‘ã€‚\n\n")
        f.write("| æ’å | é—œéµå­— | å‡ºç¾æ¬¡æ•¸ |\n")
        f.write("|---|---|---|\n")
        for i, (kw, count) in enumerate(keyword_counts.most_common(50), 1):
            f.write(f"| {i} | {kw} | {count} |\n")
            
        f.write("\n---\n\n")
        f.write("## 3. æ­·å±†è©¦é¡Œå½™æ•´ (Original Questions)\n\n")
        
        # Group by year
        current_year = -1
        for q in all_questions:
            if q['year'] != current_year:
                f.write(f"### {q['year']} å¹´åœ°æ–¹ç‰¹è€ƒä¸‰ç­‰\n\n")
                current_year = q['year']
            
            meta = q['metadata']
            f.write(f"#### {q['number']}ã€({meta.get('subject', 'è³‡è¨Šç®¡ç†')})\n")
            f.write(f"**é—œéµå­—**: {', '.join(q['keywords'])}\n\n")
            f.write(f"```text\n{q['content']}\n```\n\n")

    print(f"\nâœ“ æ–‡ä»¶å·²ç”Ÿæˆ: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
