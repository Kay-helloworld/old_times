#!/usr/bin/env python3
"""
è³‡è¨Šç®¡ç†ç§‘ç›®é—œéµå­—æå–è…³æœ¬
æƒææ‰€æœ‰è€ƒé¡Œï¼Œæå–æ‰€æœ‰é‡è¦é—œéµå­—ï¼ˆåŒ…æ‹¬è³‡è¨Šç®¡ç†ã€è³‡è¨Šå®‰å…¨ã€è³‡æ–™çµæ§‹ç­‰å¤šé ˜åŸŸé—œéµå­—ï¼‰
"""

import re
from pathlib import Path
from collections import Counter
import jieba
import jieba.analyse

def extract_exam_metadata(text):
    """æå–è€ƒè©¦å¹´ä»½ã€è€ƒåˆ¥ã€ç­‰åˆ¥ç­‰è³‡è¨Š"""
    lines = text.split('\n')
    metadata = {
        'year': None,
        'exam_type': None,
        'level': None,
        'subject': None
    }
    
    # æå–å¹´ä»½
    year_match = re.search(r'(\d{3})å¹´', text[:200])
    if year_match:
        metadata['year'] = year_match.group(1)
    
    # æå–è€ƒåˆ¥
    exam_types = ['é«˜ç­‰è€ƒè©¦', 'æ™®é€šè€ƒè©¦', 'é—œå‹™äººå“¡', 'èº«å¿ƒéšœç¤™', 'åœ°æ–¹æ”¿åºœ', 'åœ‹è»']
    for exam_type in exam_types:
        if exam_type in text[:300]:
            metadata['exam_type'] = exam_type
            break
    
    # æå–ç­‰åˆ¥
    level_match = re.search(r'(ä¸‰ç­‰|äºŒç­‰|å››ç­‰|äº”ç­‰|ä¸‰ç´š|äºŒç´š)', text[:200])
    if level_match:
        metadata['level'] = level_match.group(1)
    
    # æå–ç§‘ç›®ï¼ˆå¾æª”åæˆ–å…§å®¹åˆ¤æ–·ï¼‰
    if 'è³‡è¨Šç®¡ç†èˆ‡è³‡é€šå®‰å…¨' in text[:200]:
        metadata['subject'] = 'è³‡è¨Šç®¡ç†èˆ‡è³‡é€šå®‰å…¨'
    elif 'è³‡è¨Šç®¡ç†' in text[:200]:
        metadata['subject'] = 'è³‡è¨Šç®¡ç†'
    
    return metadata

def clean_question_text(text):
    """æ¸…ç†é¡Œç›®æ–‡å­—ï¼Œç§»é™¤ä»£è™Ÿã€é æ¬¡ç­‰ç„¡é—œè³‡è¨Š"""
    lines = text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        # è·³éä»£è™Ÿã€é æ¬¡ã€æ³¨æ„äº‹é …ç­‰
        if any(keyword in line for keyword in ['ä»£è™Ÿï¼š', 'é æ¬¡ï¼š', 'â€»æ³¨æ„ï¼š', 'ä¸å¿…æŠ„é¡Œ', 'åº§è™Ÿï¼š']):
            continue
        # è·³éå‰å¹¾è¡Œçš„è€ƒè©¦è³‡è¨Š
        if any(keyword in line for keyword in ['è€ƒ è©¦ åˆ¥ï¼š', 'ç­‰ åˆ¥ï¼š', 'é¡ ç§‘ï¼š', 'ç§‘ ç›®ï¼š', 'è€ƒè©¦æ™‚é–“ï¼š']):
            continue
        cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines)

def parse_questions(text):
    """è§£æé¡Œç›®ï¼ŒæŒ‰é¡Œè™Ÿåˆ†å‰²"""
    # ç§»é™¤å‰é¢çš„å…ƒè³‡è¨Š
    cleaned_text = clean_question_text(text)
    
    # æŒ‰ä¸­æ–‡æ•¸å­—é¡Œè™Ÿåˆ†å‰²ï¼ˆä¸€ã€äºŒã€ä¸‰ã€å››ã€äº”ç­‰ï¼‰
    question_pattern = r'(ä¸€|äºŒ|ä¸‰|å››|äº”)ã€'
    parts = re.split(question_pattern, cleaned_text)
    
    questions = []
    for i in range(1, len(parts), 2):
        if i+1 < len(parts):
            number = parts[i]
            content = parts[i+1].strip()
            if content:
                questions.append({
                    'number': number,
                    'content': content
                })
    
    return questions

def extract_keywords_jieba(text, top_k=30):
    """ä½¿ç”¨ jieba æå–é—œéµå­—"""
    # ä½¿ç”¨ TF-IDF æå–é—œéµå­—
    keywords_tfidf = jieba.analyse.extract_tags(text, topK=top_k, withWeight=True)
    
    # ä½¿ç”¨ TextRank æå–é—œéµå­—
    keywords_textrank = jieba.analyse.textrank(text, topK=top_k, withWeight=True)
    
    return keywords_tfidf, keywords_textrank

def extract_technical_terms(text):
    """å¾æ–‡æœ¬ä¸­æå–æŠ€è¡“è¡“èªï¼ˆè‹±æ–‡ç¸®å¯«ã€å°ˆæœ‰åè©ç­‰ï¼‰"""
    technical_terms = []
    
    # æå–è‹±æ–‡ç¸®å¯«ï¼ˆ2-6å€‹å¤§å¯«å­—æ¯ï¼‰
    acronyms = re.findall(r'\b[A-Z]{2,6}\b', text)
    technical_terms.extend(acronyms)
    
    # æå–è‹±æ–‡å°ˆæœ‰åè©ï¼ˆé¦–å­—æ¯å¤§å¯«çš„å–®è©æˆ–è©çµ„ï¼‰
    proper_nouns = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', text)
    technical_terms.extend(proper_nouns)
    
    # æå–ç‰¹å®šæ¨¡å¼çš„æŠ€è¡“è¡“èª
    # å¦‚ï¼šIPv4, IPv6, Wi-Fi, HTTP/HTTPS ç­‰
    special_terms = re.findall(r'\b(?:IPv[46]|Wi-Fi|HTTP[S]?|FTP[S]?|SSH|SSL|TLS|TCP|UDP|DNS|DHCP|SQL|NoSQL|AI|ML|IoT|5G|4G)\b', text, re.IGNORECASE)
    technical_terms.extend(special_terms)
    
    return technical_terms

def categorize_by_domain(keywords):
    """æ ¹æ“šé ˜åŸŸå°é—œéµå­—é€²è¡Œåˆæ­¥åˆ†é¡"""
    domains = {
        'è³‡è¨Šç®¡ç†': [],
        'è³‡è¨Šå®‰å…¨': [],
        'è³‡æ–™çµæ§‹': [],
        'è³‡æ–™åº«': [],
        'ç¶²è·¯æŠ€è¡“': [],
        'ç³»çµ±é–‹ç™¼': [],
        'æ–°èˆˆæŠ€è¡“': [],
        'ç®¡ç†æ¦‚å¿µ': [],
        'å…¶ä»–': []
    }
    
    # å®šç¾©å„é ˜åŸŸçš„ç‰¹å¾µé—œéµå­—
    domain_keywords = {
        'è³‡è¨Šç®¡ç†': ['è³‡è¨Šç®¡ç†', 'MIS', 'ERP', 'CRM', 'SCM', 'ä¼æ¥­è³‡æº', 'å°ˆæ¡ˆç®¡ç†', 'PMBOK', 'è»Ÿé«”é–‹ç™¼', 'SDLC', 
                  'ITæ²»ç†', 'COBIT', 'ITIL', 'æµç¨‹', 'ç­–ç•¥', 'çµ„ç¹”', 'ç®¡ç†è³‡è¨Šç³»çµ±'],
        'è³‡è¨Šå®‰å…¨': ['åŠ å¯†', 'å¯†ç¢¼', 'Encryption', 'AES', 'DES', 'RSA', 'é˜²ç«ç‰†', 'Firewall', 'IDS', 'IPS',
                  'å…¥ä¾µ', 'æ¼æ´', 'Vulnerability', 'æ”»æ“Š', 'Attack', 'å®‰å…¨', 'Security', 'è³‡å®‰', 'ISO27001',
                  'é¢¨éšª', 'ç¨½æ ¸', 'æ†‘è­‰', 'Certificate', 'æ•¸ä½ç°½ç« ', 'é›œæ¹Š', 'Hash', 'MD5', 'SHA'],
        'è³‡æ–™çµæ§‹': ['é™£åˆ—', 'Array', 'éˆçµ', 'Linked List', 'å †ç–Š', 'Stack', 'ä½‡åˆ—', 'Queue', 'æ¨¹', 'Tree',
                  'åœ–', 'Graph', 'æ’åº', 'Sort', 'æœå°‹', 'Search', 'é›œæ¹Š', 'Hash', 'æ¼”ç®—æ³•', 'Algorithm',
                  'è¤‡é›œåº¦', 'Complexity', 'Big-O', 'DFS', 'BFS'],
        'è³‡æ–™åº«': ['è³‡æ–™åº«', 'Database', 'SQL', 'NoSQL', 'æ­£è¦åŒ–', 'Normalization', 'äº¤æ˜“', 'Transaction',
                'ç´¢å¼•', 'Index', 'ER Model', 'é—œè¯å¼', 'Relational', 'ACID', 'JOIN', 'MongoDB', 'Redis'],
        'ç¶²è·¯æŠ€è¡“': ['ç¶²è·¯', 'Network', 'TCP', 'UDP', 'IP', 'OSI', 'Layer', 'è·¯ç”±', 'Router', 'äº¤æ›å™¨', 'Switch',
                  'DNS', 'DHCP', 'HTTP', 'HTTPS', 'VPN', 'VLAN', 'å­ç¶²è·¯', 'Subnet', 'å°åŒ…', 'Packet'],
        'ç³»çµ±é–‹ç™¼': ['UML', 'éœ€æ±‚åˆ†æ', 'ç³»çµ±åˆ†æ', 'ç³»çµ±è¨­è¨ˆ', 'æ¸¬è©¦', 'Testing', 'é»‘ç®±', 'ç™½ç®±', 'Agile', 'Scrum',
                  'DevOps', 'ç‰ˆæœ¬æ§åˆ¶', 'Git', 'è»Ÿé«”å·¥ç¨‹', 'Software Engineering', 'ç‰©ä»¶å°å‘', 'OOP'],
        'æ–°èˆˆæŠ€è¡“': ['é›²ç«¯', 'Cloud', 'AWS', 'Azure', 'AI', 'äººå·¥æ™ºæ…§', 'æ©Ÿå™¨å­¸ç¿’', 'Machine Learning', 'Deep Learning',
                  'ChatGPT', 'IoT', 'ç‰©è¯ç¶²', 'å€å¡Šéˆ', 'Blockchain', 'å¤§æ•¸æ“š', 'Big Data', '5G', 'å…ƒå®‡å®™'],
        'ç®¡ç†æ¦‚å¿µ': ['ç­–ç•¥', 'è¦åŠƒ', 'ç®¡ç†', 'é ˜å°', 'çµ„ç¹”', 'æ§åˆ¶', 'è©•ä¼°', 'æ•ˆç›Š', 'ROI', 'KPI', 'BSC', 'å¹³è¡¡è¨ˆåˆ†å¡',
                  'å°ˆæ¡ˆ', 'Project', 'é¢¨éšªç®¡ç†', 'è®Šæ›´ç®¡ç†', 'æµç¨‹æ”¹å–„', 'BPR']
    }
    
    for keyword in keywords:
        categorized = False
        for domain, domain_kws in domain_keywords.items():
            # æª¢æŸ¥é—œéµå­—æ˜¯å¦åŒ…å«æˆ–è¢«åŒ…å«åœ¨é ˜åŸŸé—œéµå­—ä¸­
            if any(kw.lower() in keyword.lower() or keyword.lower() in kw.lower() for kw in domain_kws):
                domains[domain].append(keyword)
                categorized = True
                break
        
        if not categorized:
            domains['å…¶ä»–'].append(keyword)
    
    return domains

def main():
    # è¨­å®šè·¯å¾‘
    current_dir = Path(__file__).parent
    text_dir = current_dir / 'processed_text'
    output_dir = current_dir / 'analysis_reports'
    output_dir.mkdir(exist_ok=True)
    
    # ç²å–æ‰€æœ‰æ–‡å­—æª”æ¡ˆ
    text_files = sorted(text_dir.glob('*.txt'))
    
    print(f"æ‰¾åˆ° {len(text_files)} å€‹æ–‡å­—æª”æ¡ˆ")
    print("é–‹å§‹æå–é—œéµå­—...\n")
    
    # å„²å­˜æ‰€æœ‰è€ƒé¡Œè³‡è¨Š
    all_questions = []
    all_metadata = []
    all_keywords_counter = Counter()
    all_technical_terms = Counter()
    
    # åˆä½µæ‰€æœ‰é¡Œç›®å…§å®¹ç”¨æ–¼æ•´é«”åˆ†æ
    combined_text = ""
    
    for i, text_file in enumerate(text_files, 1):
        with open(text_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–å…ƒè³‡è¨Š
        metadata = extract_exam_metadata(content)
        metadata['filename'] = text_file.name
        all_metadata.append(metadata)
        
        # è§£æé¡Œç›®
        questions = parse_questions(content)
        
        print(f"[{i}/{len(text_files)}] {text_file.name}")
        print(f"  å¹´ä»½: {metadata['year']}, è€ƒåˆ¥: {metadata['exam_type']}, ç­‰åˆ¥: {metadata['level']}")
        print(f"  é¡Œç›®æ•¸: {len(questions)}")
        
        # è™•ç†æ¯å€‹é¡Œç›®
        for q in questions:
            q['metadata'] = metadata
            all_questions.append(q)
            combined_text += q['content'] + "\n\n"
            
            # æå–æŠ€è¡“è¡“èª
            tech_terms = extract_technical_terms(q['content'])
            all_technical_terms.update(tech_terms)
        
        print()
    
    print(f"\nç¸½å…±æå– {len(all_questions)} é“é¡Œç›®\n")
    print("æ­£åœ¨ä½¿ç”¨ jieba åˆ†æé—œéµå­—...")
    
    # ä½¿ç”¨ jieba æå–æ•´é«”é—œéµå­—
    keywords_tfidf, keywords_textrank = extract_keywords_jieba(combined_text, top_k=100)
    
    # åˆä½µå…©ç¨®æ–¹æ³•çš„çµæœ
    all_keywords = {}
    for kw, weight in keywords_tfidf:
        all_keywords[kw] = all_keywords.get(kw, 0) + weight
    for kw, weight in keywords_textrank:
        all_keywords[kw] = all_keywords.get(kw, 0) + weight
    
    # æ’åº
    sorted_keywords = sorted(all_keywords.items(), key=lambda x: x[1], reverse=True)
    
    print(f"æå–åˆ° {len(sorted_keywords)} å€‹é—œéµå­—\n")
    
    # å°é—œéµå­—é€²è¡Œé ˜åŸŸåˆ†é¡
    top_keywords = [kw for kw, _ in sorted_keywords[:200]]  # å–å‰200å€‹é—œéµå­—
    categorized_keywords = categorize_by_domain(top_keywords)
    
    # ç”Ÿæˆå ±å‘Š
    report = f"""# è³‡è¨Šç®¡ç†ç§‘ç›® - é—œéµå­—æå–å ±å‘Š

**åˆ†ææ™‚é–“**: {Path(__file__).parent.name}

**åˆ†ææª”æ¡ˆæ•¸é‡**: {len(text_files)} ä»½

**åˆ†æé¡Œç›®æ•¸é‡**: {len(all_questions)} é¡Œ

**åˆ†æå¹´ä»½ç¯„åœ**: 104-114å¹´

---

## ğŸ“‹ è€ƒé¡Œçµ±è¨ˆ

### å¹´ä»½åˆ†å¸ƒ

"""
    
    # çµ±è¨ˆå¹´ä»½åˆ†å¸ƒ
    year_counter = Counter(m['year'] for m in all_metadata if m['year'])
    for year in sorted(year_counter.keys(), reverse=True):
        report += f"- {year}å¹´: {year_counter[year]} ä»½\n"
    
    report += """
### è€ƒåˆ¥åˆ†å¸ƒ

"""
    
    # çµ±è¨ˆè€ƒåˆ¥åˆ†å¸ƒ
    exam_type_counter = Counter(m['exam_type'] for m in all_metadata if m['exam_type'])
    for exam_type, count in exam_type_counter.most_common():
        report += f"- {exam_type}: {count} ä»½\n"
    
    report += """

---

## ğŸ”‘ Top 100 é—œéµå­—ï¼ˆæŒ‰æ¬Šé‡æ’åºï¼‰

| æ’å | é—œéµå­— | æ¬Šé‡ |
| :---: | :--- | :--- |
"""
    
    for i, (kw, weight) in enumerate(sorted_keywords[:100], 1):
        report += f"| {i} | {kw} | {weight:.4f} |\n"
    
    report += """

---

## ğŸ·ï¸ é ˜åŸŸåˆ†é¡é—œéµå­—

"""
    
    for domain, keywords in categorized_keywords.items():
        if keywords:
            report += f"### {domain} ({len(keywords)} å€‹)\n\n"
            # æ¯è¡Œæœ€å¤š8å€‹é—œéµå­—
            for i in range(0, len(keywords), 8):
                chunk = keywords[i:i+8]
                report += "- " + " | ".join(chunk) + "\n"
            report += "\n"
    
    report += """

---

## ğŸ’» æŠ€è¡“è¡“èªçµ±è¨ˆï¼ˆè‹±æ–‡ç¸®å¯«èˆ‡å°ˆæœ‰åè©ï¼‰

### Top 50 æŠ€è¡“è¡“èª

"""
    
    for term, count in all_technical_terms.most_common(50):
        report += f"- {term}: {count} æ¬¡\n"
    
    report += """

---

## ğŸ“ è³‡è¨Šå®‰å…¨ç›¸é—œé¡Œç›®æ¨™è¨˜

ä»¥ä¸‹è€ƒå·åŒ…å«ã€Œè³‡è¨Šå®‰å…¨ã€ã€ã€Œè³‡é€šå®‰å…¨ã€ç­‰å­—çœ¼ï¼Œå¯èƒ½åŒ…å«è³‡å®‰è€ƒé¡Œï¼š

"""
    
    # æ¨™è¨˜å¯èƒ½åŒ…å«è³‡å®‰é¡Œç›®çš„è€ƒå·
    infosec_files = []
    for metadata in all_metadata:
        if metadata['subject'] == 'è³‡è¨Šç®¡ç†èˆ‡è³‡é€šå®‰å…¨':
            infosec_files.append(metadata)
    
    for m in infosec_files:
        report += f"- {m['year']}å¹´ {m['exam_type']} {m['level']} - {m['filename']}\n"
    
    report += f"""

**å°è¨ˆ**: {len(infosec_files)} ä»½è€ƒå·å¯èƒ½åŒ…å«è³‡å®‰èˆ‡è³‡è¨Šç®¡ç†è¤‡åˆé¡Œå‹

---

## ğŸ’¡ èªªæ˜

- **åˆ†ææ–¹æ³•**: ä½¿ç”¨ jieba ä¸­æ–‡åˆ†è© + TF-IDF èˆ‡ TextRank æ¼”ç®—æ³•æå–é—œéµå­—
- **æŠ€è¡“è¡“èª**: ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–è‹±æ–‡ç¸®å¯«ã€å°ˆæœ‰åè©ç­‰
- **é ˜åŸŸåˆ†é¡**: æ ¹æ“šé å®šç¾©çš„é ˜åŸŸç‰¹å¾µé—œéµå­—é€²è¡Œåˆ†é¡
- **è³‡å®‰é¡Œç›®**: æ¨™è¨˜ç§‘ç›®åç¨±åŒ…å«ã€Œè³‡é€šå®‰å…¨ã€çš„è€ƒå·ï¼Œé€™äº›è€ƒå·å¯èƒ½åŒæ™‚åŒ…å«è³‡è¨Šç®¡ç†èˆ‡è³‡è¨Šå®‰å…¨çš„é¡Œç›®

## ğŸ“Œ ä¸‹ä¸€æ­¥å»ºè­°

1. **é—œéµå­—ç²¾ç…‰**: æ ¹æ“šæ­¤å ±å‘Šï¼Œç²¾ç…‰å’Œè£œå……å„é ˜åŸŸçš„é—œéµå­—åˆ—è¡¨
2. **é¡Œç›®åˆ†é¡**: ä½¿ç”¨ç²¾ç…‰å¾Œçš„é—œéµå­—å°æ‰€æœ‰é¡Œç›®é€²è¡Œåˆ†é¡
3. **è¤‡åˆé¡Œå‹è™•ç†**: å°æ–¼åŒ…å«å¤šå€‹é ˜åŸŸé—œéµå­—çš„é¡Œç›®ï¼Œæ¨™è¨˜ç‚ºè¤‡åˆé¡Œå‹
4. **è³‡å®‰é¡Œç›®åˆ†é›¢**: å°æ–¼ã€Œè³‡è¨Šç®¡ç†èˆ‡è³‡é€šå®‰å…¨ã€ç§‘ç›®çš„è€ƒå·ï¼Œéœ€è¦ç‰¹åˆ¥åˆ†æå“ªäº›æ˜¯ç´”è³‡å®‰é¡Œç›®

"""
    
    # å„²å­˜å ±å‘Š
    output_file = output_dir / 'keyword_extraction_report.md'
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ“ å ±å‘Šå·²ç”Ÿæˆï¼š{output_file}\n")
    
    # åŒæ™‚ç”Ÿæˆä¸€å€‹ç°¡åŒ–çš„é—œéµå­—æ¸…å–®ä¾›å¾ŒçºŒä½¿ç”¨
    keywords_file = output_dir / 'extracted_keywords.txt'
    with open(keywords_file, 'w', encoding='utf-8') as f:
        f.write("# è³‡è¨Šç®¡ç†ç§‘ç›® - æå–çš„é—œéµå­—åˆ—è¡¨\n\n")
        f.write("## æŒ‰æ¬Šé‡æ’åº (Top 200)\n\n")
        for i, (kw, weight) in enumerate(sorted_keywords[:200], 1):
            f.write(f"{i}. {kw} ({weight:.4f})\n")
        
        f.write("\n## æŒ‰é ˜åŸŸåˆ†é¡\n\n")
        for domain, keywords in categorized_keywords.items():
            if keywords:
                f.write(f"### {domain}\n\n")
                f.write(", ".join(keywords))
                f.write("\n\n")
    
    print(f"âœ“ é—œéµå­—æ¸…å–®å·²ç”Ÿæˆï¼š{keywords_file}\n")

if __name__ == '__main__':
    main()
