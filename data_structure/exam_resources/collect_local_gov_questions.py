#!/usr/bin/env python3
"""
è³‡æ–™çµæ§‹åœ°æ–¹ç‰¹è€ƒä¸‰ç´šè©¦é¡Œæ”¶é›†è…³æœ¬
"""
import os
import re
import glob
from collections import Counter
import jieba
import jieba.analyse

# Configuration
SOURCE_DIR = "/Users/kaylo/Documents/ç¨‹å¼ç›¸é—œ/antigravity/data_structure/exam_resources/processed_text"
OUTPUT_FILE = "/Users/kaylo/Documents/ç¨‹å¼ç›¸é—œ/antigravity/data_structure/essay_guides/local_gov_exam_questions.md"

# Custom dictionary for data structure terms
CUSTOM_DICT = [
    # è³‡æ–™çµæ§‹åŸºç¤
    "è³‡æ–™çµæ§‹", "Data Structure", "Array", "é™£åˆ—",
    "Linked List", "éˆçµä¸²åˆ—", "é€£çµä¸²åˆ—", "ä¸²åˆ—",
    "Stack", "å †ç–Š", "Queue", "ä½‡åˆ—",
    "Tree", "æ¨¹", "Binary Tree", "äºŒå…ƒæ¨¹",
    "Graph", "åœ–", "åœ–å½¢",
    
    # æ¨¹ç‹€çµæ§‹
    "BST", "Binary Search Tree", "äºŒå…ƒæœå°‹æ¨¹",
    "AVL Tree", "AVL æ¨¹", "å¹³è¡¡æ¨¹",
    "Red-Black Tree", "ç´…é»‘æ¨¹",
    "B Tree", "B-Tree", "B æ¨¹",
    "B+ Tree", "B+æ¨¹",
    "Heap", "å †ç©", "Min Heap", "Max Heap",
    "Priority Queue", "å„ªå…ˆä½‡åˆ—",
    "Huffman Tree", "éœå¤«æ›¼æ¨¹",
    
    # åœ–è«–
    "BFS", "Breadth-First Search", "å»£åº¦å„ªå…ˆæœå°‹",
    "DFS", "Depth-First Search", "æ·±åº¦å„ªå…ˆæœå°‹",
    "Dijkstra", "æœ€çŸ­è·¯å¾‘",
    "MST", "Minimum Spanning Tree", "æœ€å°ç”Ÿæˆæ¨¹",
    "Kruskal", "Prim",
    
    # æ’åºæ¼”ç®—æ³•
    "Sorting", "æ’åº",
    "Bubble Sort", "æ°£æ³¡æ’åº",
    "Selection Sort", "é¸æ“‡æ’åº",
    "Insertion Sort", "æ’å…¥æ’åº",
    "Merge Sort", "åˆä½µæ’åº",
    "Quick Sort", "å¿«é€Ÿæ’åº",
    "Heap Sort", "å †ç©æ’åº",
    "Radix Sort", "åŸºæ•¸æ’åº",
    "Counting Sort", "è¨ˆæ•¸æ’åº",
    
    # æœå°‹æ¼”ç®—æ³•
    "Searching", "æœå°‹",
    "Linear Search", "ç·šæ€§æœå°‹",
    "Binary Search", "äºŒå…ƒæœå°‹",
    "Hashing", "é›œæ¹Š", "Hash Table", "é›œæ¹Šè¡¨",
    
    # æ¼”ç®—æ³•åˆ†æ
    "Time Complexity", "æ™‚é–“è¤‡é›œåº¦",
    "Space Complexity", "ç©ºé–“è¤‡é›œåº¦",
    "Big O", "Big-O", "O(n)", "O(log n)",
    "Recursion", "éè¿´", "Iteration", "è¿´åœˆ",
    
    # é€²éšä¸»é¡Œ
    "Dynamic Programming", "å‹•æ…‹è¦åŠƒ", "DP",
    "Greedy", "è²ªå©ªæ¼”ç®—æ³•",
    "Divide and Conquer", "åˆ†æ²»æ³•",
    "Backtracking", "å›æº¯æ³•",
    
    # ç¨‹å¼èªè¨€
    "C", "C++", "Java", "Python", "C#",
]

def setup_jieba():
    for word in CUSTOM_DICT:
        jieba.add_word(word)

def extract_year(filename):
    # Handle "112 å¹´ç‰¹ç¨®è€ƒè©¦" format
    match = re.search(r'(\d{3})\s*å¹´', filename)
    if match:
        return int(match.group(1))
    return 0

def is_target_file(content):
    # Check for "åœ°æ–¹" and "ä¸‰ç­‰" or "ä¸‰ç´š"
    if ("åœ°æ–¹" in content or "åœ°æ–¹æ”¿åºœ" in content) and ("ä¸‰ç­‰" in content or "ä¸‰ç´š" in content):
        return True
    return False

def extract_questions_from_file(filepath):
    filename = os.path.basename(filepath)
    year = extract_year(filename)
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Check if this is a target file
        if not is_target_file(content):
            return []
        
        # Extract metadata
        metadata = {'year': year, 'filename': filename}
        
        # Extract exam type
        metadata['exam_type'] = 'åœ°æ–¹ç‰¹è€ƒ'
        
        # Extract level
        if 'ä¸‰ç­‰' in content:
            metadata['level'] = 'ä¸‰ç­‰'
        elif 'ä¸‰ç´š' in content:
            metadata['level'] = 'ä¸‰ç­‰'
        
        # Extract subject
        metadata['subject'] = 'è³‡æ–™çµæ§‹'
        
        # Parse questions using Chinese numerals
        question_pattern = r'^(ä¸€|äºŒ|ä¸‰|å››|äº”)ã€'
        lines = content.split('\n')
        
        questions = []
        current_question = None
        current_number = None
        
        for line in lines:
            line = line.strip()
            # Skip metadata lines
            if any(skip in line for skip in ['ä»£è™Ÿï¼š', 'é æ¬¡ï¼š', 'â€»æ³¨æ„', 'åº§è™Ÿï¼š', 'è€ƒ è©¦ åˆ¥', 'ç­‰ åˆ¥', 'é¡ ç§‘', 'ç§‘ ç›®', 'è€ƒè©¦æ™‚é–“']):
                continue
            
            match = re.match(question_pattern, line)
            if match:
                # Save previous question
                if current_question:
                    questions.append({
                        'year': year,
                        'number': current_number,
                        'content': '\n'.join(current_question),
                        'metadata': metadata
                    })
                
                # Start new question
                current_number = match.group(1)
                current_question = [line]
            elif current_question is not None and line:
                current_question.append(line)
        
        # Add last question
        if current_question:
            questions.append({
                'year': year,
                'number': current_number,
                'content': '\n'.join(current_question),
                'metadata': metadata
            })
            
    except Exception as e:
        print(f"Error reading {filename}: {e}")
        return []
        
    return questions

def extract_keywords(text, top_k=10):
    keywords = jieba.analyse.extract_tags(text, topK=top_k, allowPOS=('n', 'eng', 'v', 'vn'))
    filtered = [k for k in keywords if len(k) > 1 or k.upper() in CUSTOM_DICT]
    return filtered

def main():
    setup_jieba()
    
    all_questions = []
    files = glob.glob(os.path.join(SOURCE_DIR, "*.txt"))
    
    print(f"æƒæ {len(files)} å€‹æª”æ¡ˆ...")
    
    for filepath in files:
        qs = extract_questions_from_file(filepath)
        all_questions.extend(qs)
        if qs:
            print(f"  âœ“ {os.path.basename(filepath)}: {len(qs)} é¡Œ")
    
    # Sort questions by year (descending)
    all_questions.sort(key=lambda x: x['year'], reverse=True)
    
    print(f"\nç¸½å…±æ‰¾åˆ° {len(all_questions)} é¡Œåœ°æ–¹ç‰¹è€ƒä¸‰ç­‰è³‡æ–™çµæ§‹è©¦é¡Œ")
    
    # Analyze keywords
    all_keywords = []
    
    for q in all_questions:
        text = q['content']
        keywords = extract_keywords(text)
        
        # Manual check for custom terms
        for term in CUSTOM_DICT:
            if term.lower() in text.lower() and term not in keywords:
                keywords.append(term)
        
        q['keywords'] = keywords
        all_keywords.extend(keywords)
        
    keyword_counts = Counter(all_keywords)
    
    # Ensure output directory exists
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    
    # Generate Markdown
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write("# åœ°æ–¹ç‰¹è€ƒä¸‰ç­‰ è³‡æ–™çµæ§‹ æ­·å±†è©¦é¡Œå½™æ•´èˆ‡åˆ†æ\n\n")
        f.write("> æœ¬æ–‡ä»¶å½™æ•´ **åœ°æ–¹ç‰¹è€ƒä¸‰ç­‰** è³‡æ–™çµæ§‹ç§‘ç›®æ­·å±†è€ƒé¡Œï¼Œæä¾›å®Œæ•´åŸé¡Œèˆ‡é—œéµå­—åˆ†æï¼Œæ˜¯å‚™è€ƒè³‡æ–™çµæ§‹è€ƒè©¦çš„é‡è¦åƒè€ƒè³‡æºã€‚\n\n")
        f.write("---\n\n")
        
        # Statistics section will be added separately
        f.write("## 1. ğŸ“Š é¡Œç›®ç¸½è¦½\n\n")
        f.write("*çµ±è¨ˆæ•¸æ“šæº–å‚™ä¸­...*\n\n")
        f.write("---\n\n")
        
        f.write("## 2. é—œéµå­—åˆ†æ (Keyword Analysis)\n\n")
        f.write("ä»¥ä¸‹ç‚ºæ­·å±†è©¦é¡Œä¸­å‡ºç¾é »ç‡æœ€é«˜çš„é—œéµå­—ï¼Œå¯ä½œç‚ºé‡é»è¤‡ç¿’æ–¹å‘ã€‚\n\n")
        f.write("| æ’å | é—œéµå­— | å‡ºç¾æ¬¡æ•¸ |\n")
        f.write("|---|---|---|\n")
        for i, (kw, count) in enumerate(keyword_counts.most_common(50), 1):
            f.write(f"| {i} | {kw} | {count} |\n")
            
        f.write("\n---\n\n")
        f.write("## 3. æ­·å±†è©¦é¡Œå½™æ•´ (Original Questions)\n\n")
        
        # Group by year
        current_year = -1
        for q in all_questions:
            if q['year'] != current_year:
                f.write(f"### {q['year']} å¹´åœ°æ–¹ç‰¹è€ƒä¸‰ç­‰\n\n")
                current_year = q['year']
            
            meta = q['metadata']
            f.write(f"#### {q['number']}ã€({meta.get('subject', 'è³‡æ–™çµæ§‹')})\n")
            f.write(f"**é—œéµå­—**: {', '.join(q['keywords'])}\n\n")
            f.write(f"```text\n{q['content']}\n```\n\n")

    print(f"\nâœ“ æ–‡ä»¶å·²ç”Ÿæˆ: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
